{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plant Growth Classification Prediction Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook ini digunakan untuk membuat file .py sebagai module dari transform, trainer, dan tuner, serta untuk membuat file.py components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilakukan import libraries untuk libraries yang dibutuhkan pada proyek ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import sys\n",
    "from tfx.components import (\n",
    "    CsvExampleGen, \n",
    "    StatisticsGen, \n",
    "    SchemaGen, \n",
    "    ExampleValidator, \n",
    "    Transform, \n",
    "    Trainer,\n",
    "    Tuner,\n",
    "    Evaluator,\n",
    "    Pusher\n",
    ")\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2 \n",
    "from tfx.types import Channel\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
    "    LatestBlessedModelStrategy)\n",
    "from typing import Text\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from tfx.orchestration.metadata import sqlite_metadata_connection_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat directory dari pipeline, module, dan output component pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tasyaputrialiya-pipeline\"\n",
    " \n",
    "# pipeline inputs\n",
    "DATA_ROOT = \"data\"\n",
    "MODULE_COMPONENTS = \"modules/components.py\"\n",
    "TRANSFORM_MODULE_FILE = \"modules/plant_growth_transform.py\"\n",
    "TRAINER_MODULE_FILE = \"modules/plant_growth_trainer.py\"\n",
    "TUNER_MODULE_FILE = \"modules/plant_growth_tuner.py\"\n",
    "LOCAL_PIPELINE=\"local_pipeline.py\"\n",
    "# requirement_file = os.path.join(root, \"requirements.txt\")\n",
    "\n",
    "OUTPUT_BASE = \"output\"\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat file TRANFORM_MODULE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code di bawah digunakan untuk membuat file transform module file yang akan digunakan sebagai module file dalam transform. Isi dari bagian module transform ini adalah function untuk mengubah nama dari variabel data dan juga memastikan tipe data yang akan digunakan sesuai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/plant_growth_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "LABEL_KEY = \"Growth_Milestone\"\n",
    "FEATURE_KEYS = ['Sunlight_Hours', 'Temperature', 'Humidity']\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess input features into transformed features\n",
    "\n",
    "    Args:\n",
    "        inputs: map from feature keys to raw features.\n",
    "\n",
    "    Return:\n",
    "        outputs: map from feature keys to transformed features.    \n",
    "    \"\"\"\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    # Transform numerical features\n",
    "    for key in FEATURE_KEYS:\n",
    "        outputs[transformed_name(key)] = tft.scale_to_z_score(inputs[key])\n",
    "    \n",
    "    # Transform label\n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat file TRAINER_MODULE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code di bawah ini digunakan untuk membuat file module yang akan digunakan dalam pipeline trainer, module ini akan berisi beberapa function, yaitu \n",
    "1. **transformed_name()** untuk melakukan transformasi nama\n",
    "2. **gzip_reader_fn()** untuk memuat data dalam TFRecord\n",
    "3. **input_fn()** untuk membuat transformed_feature yang dihasilkan komponen transform\n",
    "4. **model_builder()** untuk membuat arsitektur model \n",
    "5. **_get_serve_tf_examples_fn()** untuk menjalankan tahapan preprocessing\n",
    "6. **run_fn()** untuk menjalankan proses training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/plant_growth_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "\n",
    "LABEL_KEY = \"Growth_Milestone\"\n",
    "FEATURE_KEYS = ['Sunlight_Hours', 'Temperature', 'Humidity']\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compressed data\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "def input_fn(file_pattern, \n",
    "             tf_transform_output,\n",
    "             num_epochs,\n",
    "             batch_size=64) -> tf.data.Dataset:\n",
    "    \"\"\"Get post_transform feature & create batches of data\"\"\"\n",
    "    \n",
    "    # Get post_transform feature spec\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy())\n",
    "    \n",
    "    # Create batches of data\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key=transformed_name(LABEL_KEY))\n",
    "    return dataset\n",
    "\n",
    "def model_builder():\n",
    "    \"\"\"Build machine learning model\"\"\"\n",
    "    inputs = {transformed_name(key): tf.keras.Input(shape=(1,), name=transformed_name(key), dtype=tf.float32) for key in FEATURE_KEYS}\n",
    "    x = layers.Concatenate()(list(inputs.values()))\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        \n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        \n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        \n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        \n",
    "        # Get predictions using the transformed features\n",
    "        return model(transformed_features)\n",
    "        \n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def run_fn(fn_args: FnArgs) -> None:\n",
    "    \n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, update_freq='batch'\n",
    "    )\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    # Load the transform output\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    \n",
    "    # Create batches of data\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)\n",
    "    val_set = input_fn(fn_args.eval_files, tf_transform_output, num_epochs=10)\n",
    "    \n",
    "    # Build the model\n",
    "    model = model_builder()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=train_set,\n",
    "        validation_data=val_set,\n",
    "        callbacks=[tensorboard_callback, es, mc],\n",
    "        steps_per_epoch=1000, \n",
    "        validation_steps=1000,\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
    "                                    tf.TensorSpec(\n",
    "                                    shape=[None],\n",
    "                                    dtype=tf.string,\n",
    "                                    name='examples'))\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat file TUNER_MODULE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code di bawah digunakan untuk membuat file module yang akan digunakan pada tuner dimana module ini akan berisi beberapa function yang sebagiannya menyerupai trainer, function tersebut yaitu:\n",
    "1. **transformed_name()** untuk mentransformasi nama \n",
    "2. **gzip_reader_fn()** untuk memuat data dalam TFRecord\n",
    "3. **input_fn()** untuk mengambil input\n",
    "4. **model_builder** untuk membuat model yang akan digunakan dalam tuner\n",
    "5. **tuner_fn()** untuk menjalankan tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/plant_growth_tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TUNER_MODULE_FILE}\n",
    "from typing import NamedTuple, Dict, Any, Text\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.base_tuner import BaseTuner\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow as tf\n",
    "\n",
    "TunerFnResult = NamedTuple('TunerFnResult', [('tuner', BaseTuner), ('fit_kwargs', Dict[Text, Any])])\n",
    "\n",
    "LABEL_KEY = \"Growth_Milestone\"\n",
    "FEATURE_KEYS = ['Sunlight_Hours', 'Temperature', 'Humidity']\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compressed data\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "def input_fn(file_pattern, tf_transform_output, num_epochs=None, batch_size=64):\n",
    "    \"\"\"Get post_transform feature & create batches of data\"\"\"\n",
    "    \n",
    "    # Get post_transform feature spec\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy())\n",
    "    \n",
    "    # create batches of data\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key=transformed_name(LABEL_KEY))\n",
    "    return dataset\n",
    "\n",
    "def model_builder(hp):\n",
    "    \"\"\"Build machine learning model with hyperparameters.\"\"\"\n",
    "    inputs = {transformed_name(key): tf.keras.Input(shape=(1,), name=transformed_name(key), dtype=tf.float32) for key in FEATURE_KEYS}\n",
    "    x = layers.Concatenate()(list(inputs.values()))\n",
    "    x = layers.Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32), activation='relu')(x)\n",
    "    x = layers.Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1))(x)\n",
    "    x = layers.Dense(units=hp.Int('units_2', min_value=16, max_value=64, step=16), activation='relu')(x)\n",
    "    x = layers.Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1))(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
    "    \"\"\"Build the tuner using the KerasTuner API.\"\"\"\n",
    "    \n",
    "    # Load the transformed data\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)\n",
    "    val_set = input_fn(fn_args.eval_files, tf_transform_output, num_epochs=10)\n",
    "\n",
    "    # Define the hyperband tuner\n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder,\n",
    "        objective='val_binary_accuracy',\n",
    "        max_epochs=10,\n",
    "        factor=3,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name='kt_hyperband'\n",
    "    )\n",
    "\n",
    "    # Set fit arguments for the tuner\n",
    "    early_stopping = EarlyStopping(monitor='val_binary_accuracy',  mode='max', min_delta=0.001, patience=5, verbose=1)\n",
    "\n",
    "    fit_kwargs = {\n",
    "        \"callbacks\": [early_stopping],\n",
    "        'x': train_set,\n",
    "        'validation_data': val_set,\n",
    "        'steps_per_epoch': fn_args.train_steps,\n",
    "        'validation_steps': fn_args.eval_steps\n",
    "    }\n",
    "\n",
    "    return TunerFnResult(tuner=tuner, fit_kwargs=fit_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat file MODULE_COMPONENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code di bawah digunakan untuk membuat file module yang akan menyatukan seluruh TFX components. TFX Components yang telah disatukan pada file components hingga Pusher disatukan di module components berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODULE_COMPONENTS}\n",
    "\n",
    "\"\"\"Initiate tfx pipeline components\n",
    "\"\"\"\n",
    " \n",
    "import os\n",
    " \n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (\n",
    "    CsvExampleGen, \n",
    "    StatisticsGen, \n",
    "    SchemaGen, \n",
    "    ExampleValidator, \n",
    "    Transform, \n",
    "    Trainer,\n",
    "    Tuner,\n",
    "    Evaluator,\n",
    "    Pusher\n",
    ")\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2 \n",
    "from tfx.types import Channel\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
    "    LatestBlessedModelStrategy)\n",
    " \n",
    "def init_components(\n",
    "    data_dir,\n",
    "    transform_module,\n",
    "    training_module,\n",
    "    tuner_module,\n",
    "    training_steps,\n",
    "    eval_steps,\n",
    "    serving_model_dir,\n",
    "):\n",
    "    \"\"\"Initiate tfx pipeline components\n",
    " \n",
    "    Args:\n",
    "        data_dir (str): a path to the data\n",
    "        transform_module (str): a path to the transform_module\n",
    "        training_module (str): a path to the transform_module\n",
    "        training_steps (int): number of training steps\n",
    "        eval_steps (int): number of eval steps\n",
    "        serving_model_dir (str): a path to the serving model directory\n",
    " \n",
    "    Returns:\n",
    "        TFX components\n",
    "    \"\"\"\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config = example_gen_pb2.SplitConfig(splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2)\n",
    "        ])\n",
    "    )\n",
    " \n",
    "    example_gen = CsvExampleGen(\n",
    "        input_base=data_dir, \n",
    "        output_config=output\n",
    "    )\n",
    "    \n",
    "    statistics_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs[\"examples\"]   \n",
    "    )\n",
    "    \n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"]\n",
    "    )\n",
    "    \n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "    \n",
    "    transform  = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema= schema_gen.outputs['schema'],\n",
    "        module_file=os.path.abspath(transform_module)\n",
    "    )\n",
    "    \n",
    "    trainer  = Trainer(\n",
    "        module_file=os.path.abspath(training_module),\n",
    "        examples = transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=['train'],\n",
    "            num_steps=training_steps),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=['eval'], \n",
    "            num_steps=eval_steps)\n",
    "    )\n",
    "    \n",
    "    tuner = Tuner(\n",
    "    module_file=os.path.abspath(tuner_module),\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=500),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=100)\n",
    "    )\n",
    "\n",
    "    model_resolver = Resolver(\n",
    "        strategy_class= LatestBlessedModelStrategy,\n",
    "        model = Channel(type=Model),\n",
    "        model_blessing = Channel(type=ModelBlessing)\n",
    "    ).with_id('Latest_blessed_model_resolver')\n",
    "    \n",
    "    label_key = \"Growth_Milestone\"  # Update this if your label key is different\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key=label_key)],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(\n",
    "                metrics=[\n",
    "                    tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "                    tfma.MetricConfig(class_name=\"AUC\"),\n",
    "                    tfma.MetricConfig(class_name=\"FalsePositives\"),\n",
    "                    tfma.MetricConfig(class_name=\"TruePositives\"),\n",
    "                    tfma.MetricConfig(class_name=\"FalseNegatives\"),\n",
    "                    tfma.MetricConfig(class_name=\"TrueNegatives\"),\n",
    "                    tfma.MetricConfig(\n",
    "                        class_name=\"BinaryAccuracy\",\n",
    "                        threshold=tfma.MetricThreshold(\n",
    "                            value_threshold=tfma.GenericValueThreshold(\n",
    "                                lower_bound={\"value\": 0.5}\n",
    "                            ),\n",
    "                            change_threshold=tfma.GenericChangeThreshold(\n",
    "                                direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                                absolute={\"value\": 0.0001},\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config)\n",
    "    \n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        model_blessing=evaluator.outputs[\"blessing\"],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    components = (\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        trainer,\n",
    "        tuner,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher\n",
    "    )\n",
    "    \n",
    "    return components"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
